{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07ba8ed1f17e40a0b6e5167ce443bf6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c7e65dff5a54b359c91c34af2d4e357",
              "IPY_MODEL_6eaf5861d35a40b8a626835b42dcbe9f",
              "IPY_MODEL_2b90cc7c349041cb8b622beae56cd708"
            ],
            "layout": "IPY_MODEL_c4d6528e14524d2a981b0efa9deddc56"
          }
        },
        "3c7e65dff5a54b359c91c34af2d4e357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c9478111e6d4645b2981cbf25f5f7f0",
            "placeholder": "​",
            "style": "IPY_MODEL_4251dedeb1fe4ffbb3e686b069543bcf",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "6eaf5861d35a40b8a626835b42dcbe9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad5eff1efd404aefbacf525e26ff0cc8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fce7c65521f347f0a675c43b5c3f8465",
            "value": 0
          }
        },
        "2b90cc7c349041cb8b622beae56cd708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5a914f4d5d043e1b98f4987dcc4d479",
            "placeholder": "​",
            "style": "IPY_MODEL_25bb2aceb4d24a82b27fac846da0c046",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "c4d6528e14524d2a981b0efa9deddc56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "2c9478111e6d4645b2981cbf25f5f7f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4251dedeb1fe4ffbb3e686b069543bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad5eff1efd404aefbacf525e26ff0cc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fce7c65521f347f0a675c43b5c3f8465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5a914f4d5d043e1b98f4987dcc4d479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25bb2aceb4d24a82b27fac846da0c046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/OctoberChang/klcpd_code.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E9Bo3FdJ0V-",
        "outputId": "ce708ac3-7734-4dff-ca04-9b70f4ee6143"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'klcpd_code'...\n",
            "remote: Enumerating objects: 235, done.\u001b[K\n",
            "remote: Total 235 (delta 0), reused 0 (delta 0), pack-reused 235\u001b[K\n",
            "Receiving objects: 100% (235/235), 43.68 MiB | 16.16 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "AKDSyD0ihs7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1ef91c-6b80-460a-e311-e58ebdd0221b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.4.post0-py3-none-any.whl (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.13.0+cu116)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2022.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 76.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (21.3)\n",
            "Collecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.9.24)\n",
            "Installing collected packages: torchmetrics, tensorboardX, lightning-utilities, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.4.2 pytorch-lightning-1.8.4.post0 tensorboardX-2.5.1 torchmetrics-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from typing import List, Tuple\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "7kAqSgwphMZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "xoh6DruEoQaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path):\n",
        "    data = sio.loadmat(data_path)\n",
        "    return data['Y'], data['L']\n",
        "\n",
        "def load_matlab_v1_log(data_path):\n",
        "    eval_log = sio.loadmat(data_path)\n",
        "    ret_dict = {'Y_tst': eval_log['Y_tst'],\n",
        "                'L_tst': eval_log['L_tst'],\n",
        "                'Y_tst_pred': eval_log['Y_tst_pred'],\n",
        "                'L_tst_pred': eval_log['err'][:, 1], # use MSE as predict score\n",
        "                'err': eval_log['err']}\n",
        "    return ret_dict\n",
        "\n",
        "def load_matlab_v2_log(data_path):\n",
        "    eval_log = sio.loadmat(data_path)\n",
        "    ret_dict = {'Y_tst': eval_log['Y_tst'],\n",
        "                'L_tst': eval_log['L_tst'],\n",
        "                'Y_tst_pred': None,\n",
        "                'L_tst_pred': eval_log['Y_tst_pred'],\n",
        "                'err': None}\n",
        "    return ret_dict\n",
        "\n",
        "def load_python_log(data_path):\n",
        "    eval_log = pickle.load(open(data_path, 'rb'))\n",
        "    ret_dict = {'Y_tst': eval_log['Y_true'],\n",
        "                'L_tst': eval_log['L_true'],\n",
        "                'Y_tst_pred': eval_log['Y_pred'],\n",
        "                'L_tst_pred': eval_log['L_pred'],\n",
        "                'err': None}\n",
        "    return ret_dict\n",
        "\n",
        "def compute_auc(eval_dict):\n",
        "    L_true = eval_dict['L_tst'].flatten()\n",
        "    L_pred = eval_dict['L_tst_pred'].flatten()\n",
        "    # print('L_true', L_true.shape, 'L_pred', L_pred.shape)\n",
        "    fp_list, tp_list, thresholds = sklearn.metrics.roc_curve(L_true, L_pred)\n",
        "    auc = sklearn.metrics.auc(fp_list, tp_list)\n",
        "    return fp_list, tp_list, auc\n",
        "\n",
        "def compute_average_roc(tprs, base_fpr):\n",
        "    tprs = np.array(tprs)\n",
        "    mean_tprs = tprs.mean(axis=0)\n",
        "    std = tprs.std(axis=0)\n",
        "\n",
        "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
        "    tprs_lower = mean_tprs - std\n",
        "    return mean_tprs\n",
        "\n",
        "def forecast_loss(eval_dict):\n",
        "    assert(eval_dict['Y_tst_pred'] is not None)\n",
        "    sqr_err = np.sum((eval_dict['Y_tst'] - eval_dict['Y_tst_pred'])**2, axis=1)\n",
        "    abs_err = np.sum(abs(eval_dict['Y_tst'] - eval_dict['Y_tst_pred']), axis=1)\n",
        "    mse_mean = np.mean(sqr_err)\n",
        "    mae_mean = np.mean(abs_err)\n",
        "    return mse_mean, mae_mean\n",
        "\n",
        "def print_auc_table(result_array, all_methods):\n",
        "    # print auc for latex table\n",
        "    print('metric', end='')\n",
        "    for i, method in enumerate(all_methods):\n",
        "        print(' & %s' % (method), end='')\n",
        "    print('')\n",
        "    print('AUC', end='')\n",
        "    for i, method in enumerate(all_methods):\n",
        "        print(' & %.4f' % (np.mean(result_array[i, :])), end='')\n",
        "    print('')"
      ],
      "metadata": {
        "id": "28D3M5NEoSwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss"
      ],
      "metadata": {
        "id": "VKUc-1M87p8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------#\n",
        "#                                          Loss                                         #\n",
        "# --------------------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "#### X = Data.Y_subspace\n",
        "\n",
        "def median_heuristic(med_sqdist, beta=0.5):\n",
        "    beta_list = [beta ** 2, beta ** 1, 1, (1.0 / beta) ** 1, (1.0 / beta) ** 2]\n",
        "    return [med_sqdist * b for b in beta_list]"
      ],
      "metadata": {
        "id": "_-7UHWyf7n-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_mmd2_loss(X_p_enc, X_f_enc, sigma_var):\n",
        "    device = X_p_enc.device\n",
        "    # some constants, TODO ask Alex\n",
        "    n_basis = 1024\n",
        "    gumbel_lmd = 1e+6\n",
        "    cnst = math.sqrt(1. / n_basis)\n",
        "    n_mixtures = sigma_var.size(0)\n",
        "    n_samples = n_basis * n_mixtures\n",
        "    batch_size, seq_len, nz = X_p_enc.size()\n",
        "\n",
        "    # gumbel trick to get masking matrix to uniformly sample sigma\n",
        "    def sample_gmm(W, batch_size):\n",
        "        U = torch.FloatTensor(batch_size * n_samples, n_mixtures).uniform_()\n",
        "        U = U.to(W.device)\n",
        "        sigma_samples = F.softmax(U * gumbel_lmd, dim=1).matmul(sigma_var)\n",
        "        W_gmm = W.mul(1. / sigma_samples.unsqueeze(1))\n",
        "        W_gmm = W_gmm.view(batch_size, n_samples, nz)\n",
        "        return W_gmm\n",
        "\n",
        "    W = torch.FloatTensor(batch_size * n_samples, nz).normal_(0, 1)\n",
        "    W = W.to(device)\n",
        "    W.requires_grad = False\n",
        "    W_gmm = sample_gmm(W, batch_size)  # batch_size x n_samples x nz\n",
        "    W_gmm = torch.transpose(W_gmm, 1, 2).contiguous()  # batch_size x nz x n_samples\n",
        "\n",
        "    XW_p = torch.bmm(X_p_enc, W_gmm)  # batch_size x seq_len x n_samples\n",
        "    XW_f = torch.bmm(X_f_enc, W_gmm)  # batch_size x seq_len x n_samples\n",
        "    z_XW_p = cnst * torch.cat((torch.cos(XW_p), torch.sin(XW_p)), 2)\n",
        "    z_XW_f = cnst * torch.cat((torch.cos(XW_f), torch.sin(XW_f)), 2)\n",
        "    batch_mmd2_rff = torch.sum((z_XW_p.mean(1) - z_XW_f.mean(1)) ** 2, 1)\n",
        "    return batch_mmd2_rff"
      ],
      "metadata": {
        "id": "ihn_FJFb7unT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "N8K7LKpO7w3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mmdLossD(X_f,\n",
        "             Y_f,\n",
        "             X_f_enc,  # real (initial)   subseq (future window)\n",
        "             Y_f_enc,  # fake (generated) subseq (future window)\n",
        "             X_p_enc,  # real (initial)   subseq (past window)\n",
        "             X_f_dec,\n",
        "             Y_f_dec,\n",
        "             lambda_ae,\n",
        "             lambda_real,\n",
        "             sigma_var):\n",
        "    # batchwise MMD2 loss between X_f and Y_f\n",
        "    D_mmd2 = batch_mmd2_loss(X_f_enc, Y_f_enc, sigma_var)\n",
        "\n",
        "    # batchwise MMD2 loss between X_p and X_f\n",
        "    mmd2_real = batch_mmd2_loss(X_p_enc, X_f_enc, sigma_var)\n",
        "\n",
        "    # reconstruction loss\n",
        "    real_L2_loss = torch.mean((X_f - X_f_dec) ** 2)\n",
        "    fake_L2_loss = torch.mean((Y_f - Y_f_dec) ** 2)\n",
        "\n",
        "    lossD = D_mmd2.mean() - lambda_ae * (real_L2_loss + fake_L2_loss) - lambda_real * mmd2_real.mean()\n",
        "\n",
        "    return lossD.mean(), mmd2_real.mean()\n"
      ],
      "metadata": {
        "id": "Bh1ffqm6hNPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Models"
      ],
      "metadata": {
        "id": "_Y2joWNK8GnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetG(nn.Module):\n",
        "    def __init__(self, args, data):\n",
        "        super(NetG, self).__init__()\n",
        "        self.wnd_dim = args.wnd_dim\n",
        "        self.var_dim = data.var_dim\n",
        "        self.D = data.D\n",
        "        self.RNN_hid_dim = args.RNN_hid_dim\n",
        "\n",
        "        self.rnn_enc_layer = nn.GRU(self.var_dim, self.RNN_hid_dim, num_layers=1, batch_first=True)\n",
        "        self.rnn_dec_layer = nn.GRU(self.var_dim, self.RNN_hid_dim, num_layers=1, batch_first=True)\n",
        "        self.fc_layer = nn.Linear(self.RNN_hid_dim, self.var_dim)\n",
        "\n",
        "    # X_p:   batch_size x wnd_dim x var_dim (Encoder input)\n",
        "    # X_f:   batch_size x wnd_dim x var_dim (Decoder input)\n",
        "    # h_t:   1 x batch_size x RNN_hid_dim\n",
        "    # noise: 1 x batch_size x RNN_hid_dim\n",
        "    def forward(self, X_p, X_f, noise):\n",
        "        X_p_enc, h_t = self.rnn_enc_layer(X_p)\n",
        "        X_f_shft = self.shft_right_one(X_f)\n",
        "        hidden = h_t + noise\n",
        "        Y_f, _ = self.rnn_dec_layer(X_f_shft, hidden)\n",
        "        output = self.fc_layer(Y_f)\n",
        "        return output\n",
        "\n",
        "    def shft_right_one(self, X):\n",
        "        X_shft = X.clone()\n",
        "        X_shft[:, 0, :].data.fill_(0)\n",
        "        X_shft[:, 1:, :] = X[:, :-1, :]\n",
        "        return X_shft\n",
        "\n",
        "\n",
        "class NetD(nn.Module):\n",
        "    def __init__(self, args, data):\n",
        "        super(NetD, self).__init__()\n",
        "\n",
        "        self.wnd_dim = args.wnd_dim\n",
        "        self.var_dim = data.var_dim\n",
        "        self.D = data.D\n",
        "        self.RNN_hid_dim = args.RNN_hid_dim\n",
        "\n",
        "        self.rnn_enc_layer = nn.GRU(self.var_dim, self.RNN_hid_dim, batch_first=True)\n",
        "        self.rnn_dec_layer = nn.GRU(self.RNN_hid_dim, self.var_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X_enc, _ = self.rnn_enc_layer(X)\n",
        "        X_dec, _ = self.rnn_dec_layer(X_enc)\n",
        "        return X_enc, X_dec"
      ],
      "metadata": {
        "id": "VsBy75dyi8_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrSfPDnlgnB8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --------------------------------------------------------------------------------------#\n",
        "#                                        Models                                        #\n",
        "# --------------------------------------------------------------------------------------#\n",
        "\n",
        "# separation for training\n",
        "def _history_future_separation(data, window):\n",
        "    history = data[:, :, :window]\n",
        "    future = data[:, :, window:2 * window]\n",
        "    return history, future\n",
        "\n",
        "class KLCPD(pl.LightningModule):\n",
        "    def __init__(\n",
        "            self,\n",
        "            netG: nn.Module,\n",
        "            netD: nn.Module,\n",
        "            args: dict,\n",
        "            train_dataset: Dataset,\n",
        "            test_dataset: Dataset,\n",
        "            num_workers: int = 2\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.netG = netG\n",
        "        self.netD = netD\n",
        "\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        \n",
        "        ####\n",
        "        '''\n",
        "        def median_heuristic(X, beta=0.5):\n",
        "            max_n = min(30000, X.shape[0])\n",
        "            D2 = euclidean_distances(X[:max_n], squared=True)\n",
        "    \n",
        "\n",
        "        '''\n",
        "        #### Added for conformity\n",
        "        #X = Data.Y_subspace\n",
        "        max_n = min(30000, Data.Y_subspace.shape[0])\n",
        "        D2 = euclidean_distances(Data.Y_subspace[:max_n], squared=True)\n",
        "        med_sqdist = np.median(D2[np.triu_indices_from(D2, k=1)])\n",
        "        self.args['sqdist'] = med_sqdist\n",
        "\n",
        "\n",
        "        sigma_list = median_heuristic(self.args['sqdist'], beta=.5)\n",
        "        self.sigma_var = torch.FloatTensor(sigma_list)\n",
        "\n",
        "        # to get predictions\n",
        "\n",
        "        ### experiment, might be that window = wnd_dim\n",
        "        self.args['window'] = self.args['wnd_dim']\n",
        "        self.window = self.args['window']\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        X = batch[0].to(torch.float32)\n",
        "        X_p, X_f = _history_future_separation(X, self.args['wnd_dim'])\n",
        "\n",
        "        X_p = X_p.reshape(-1, self.args['wnd_dim'], self.args['data_dim'])\n",
        "        X_f = X_f.reshape(-1, self.args['wnd_dim'], self.args['data_dim'])\n",
        "\n",
        "        batch_size = X_p.size(0)\n",
        "\n",
        "        X_p_enc, _ = self.netD(X_p)\n",
        "        X_f_enc, _ = self.netD(X_f)\n",
        "\n",
        "        Y_pred = batch_mmd2_loss(X_p_enc, X_f_enc, self.sigma_var.to(self.device))\n",
        "\n",
        "        return Y_pred\n",
        "\n",
        "    # Alternating schedule for optimizer steps (e.g. GANs)\n",
        "    def optimizer_step(\n",
        "            self,\n",
        "            epoch: int,\n",
        "            batch_idx: int,\n",
        "            optimizer: torch.optim.Optimizer,\n",
        "            optimizer_idx: int,\n",
        "            optimizer_closure,\n",
        "            on_tpu: bool = False,\n",
        "            using_native_amp: bool = False,\n",
        "            using_lbfgs: bool = False\n",
        "    ):\n",
        "        # update generator every CRITIC_ITERS steps\n",
        "        if optimizer_idx == 0:\n",
        "            if (batch_idx + 1) % self.args['CRITIC_ITERS'] == 0:\n",
        "                # the closure (which includes the `training_step`) will be executed by `optimizer.step`\n",
        "                optimizer.step(closure=optimizer_closure)\n",
        "            else:\n",
        "                # call the closure by itself to run `training_step` + `backward` without an optimizer step\n",
        "                optimizer_closure()\n",
        "\n",
        "        # update discriminator every step\n",
        "        if optimizer_idx == 1:\n",
        "            for p in self.netD.rnn_enc_layer.parameters():\n",
        "                p.data.clamp_(-self.args['weight_clip'], self.args['weight_clip'])\n",
        "            optimizer.step(closure=optimizer_closure)\n",
        "\n",
        "    def training_step(self,\n",
        "                      batch: torch.Tensor,\n",
        "                      batch_idx: int,\n",
        "                      optimizer_idx: int\n",
        "                      ) -> torch.Tensor:\n",
        "\n",
        "        # optimize discriminator (netD)\n",
        "        if optimizer_idx == 1:\n",
        "            X = batch[0].to(torch.float32)\n",
        "            X_p, X_f = _history_future_separation(X, self.args['wnd_dim'])\n",
        "\n",
        "            X_p = X_p.reshape(-1, self.args['wnd_dim'], self.args['data_dim'])\n",
        "            X_f = X_f.reshape(-1, self.args['wnd_dim'], self.args['data_dim'])\n",
        "\n",
        "            batch_size = X_p.size(0)\n",
        "\n",
        "            # real data\n",
        "            X_p_enc, X_p_dec = self.netD(X_p)\n",
        "            X_f_enc, X_f_dec = self.netD(X_f)\n",
        "\n",
        "            # fake data\n",
        "            noise = torch.FloatTensor(1, batch_size, self.args['RNN_hid_dim']).normal_(0, 1)\n",
        "            noise.requires_grad = False\n",
        "            noise = noise.to(self.device)\n",
        "\n",
        "            Y_f = self.netG(X_p, X_f, noise)\n",
        "            Y_f = self.netG(X_p, X_f, noise)\n",
        "            Y_f_enc, Y_f_dec = self.netD(Y_f)\n",
        "\n",
        "            lossD, mmd2_real = mmdLossD(X_f, Y_f, X_f_enc, Y_f_enc, X_p_enc, X_f_dec, Y_f_dec,\n",
        "                                        self.args['lambda_ae'], self.args['lambda_real'],\n",
        "                                        self.sigma_var.to(self.device))\n",
        "            lossD = (-1) * lossD\n",
        "            self.log(\"train_loss_D\", lossD, prog_bar=True)\n",
        "            self.log(\"train_mmd2_real_D\", mmd2_real, prog_bar=True)\n",
        "            return lossD\n",
        "\n",
        "        # optimize generator (netG)\n",
        "        if optimizer_idx == 0:\n",
        "            X = batch[0].to(torch.float32)\n",
        "            X_p, X_f = _history_future_separation(X, self.args['wnd_dim'])\n",
        "\n",
        "            X_p = X_p.reshape(-1, self.args['wnd_dim'], self.args['data_dim'])\n",
        "            X_f = X_f.reshape(-1, self.args['wnd_dim'], self.args['data_dim'])\n",
        "\n",
        "            batch_size = X_p.size(0)\n",
        "\n",
        "            # real data\n",
        "            X_f_enc, X_f_dec = self.netD(X_f)\n",
        "\n",
        "            # fake data\n",
        "            noise = torch.FloatTensor(1, batch_size, self.args['RNN_hid_dim']).normal_(0, 1)\n",
        "            noise.requires_grad = False\n",
        "            noise = noise.to(self.device)\n",
        "\n",
        "            Y_f = self.netG(X_p, X_f, noise)\n",
        "            Y_f_enc, Y_f_dec = self.netD(Y_f)\n",
        "\n",
        "            # batchwise MMD2 loss between X_f and Y_f\n",
        "            G_mmd2 = batch_mmd2_loss(X_f_enc, Y_f_enc, self.sigma_var.to(self.device))\n",
        "\n",
        "            lossG = G_mmd2.mean()\n",
        "            self.log(\"train_loss_G\", lossG, prog_bar=True)\n",
        "\n",
        "            return lossG\n",
        "\n",
        "    def validation_step(self,\n",
        "                        batch: torch.Tensor,\n",
        "                        batch_idx: int\n",
        "                        ) -> torch.Tensor:\n",
        "\n",
        "        X = batch[0].to(torch.float32)\n",
        "        X_p, X_f = _history_future_separation(X, self.args['wnd_dim'])\n",
        "\n",
        "        X_p = X_p.reshape(-1, self.args['wnd_dim'], self.args['data_dim'])\n",
        "        X_f = X_f.reshape(-1, self.args['wnd_dim'], self.args['data_dim'])\n",
        "\n",
        "        X_p_enc, _ = self.netD(X_p)\n",
        "        X_f_enc, _ = self.netD(X_f)\n",
        "\n",
        "        val_mmd2_real = batch_mmd2_loss(X_p_enc, X_f_enc, self.sigma_var.to(self.device))\n",
        "\n",
        "        self.log('val_mmd2_real_D', val_mmd2_real, prog_bar=True)\n",
        "\n",
        "        return val_mmd2_real\n",
        "\n",
        "    def configure_optimizers(self) -> Tuple[torch.optim.Optimizer, torch.optim.Optimizer]:\n",
        "\n",
        "        optimizerG = torch.optim.Adam(self.netG.parameters(),\n",
        "                                      lr=self.args['lr'],\n",
        "                                      weight_decay=self.args['weight_decay'])\n",
        "\n",
        "        optimizerD = torch.optim.Adam(self.netD.parameters(),\n",
        "                                      lr=self.args['lr'],\n",
        "                                      weight_decay=self.args['weight_decay'])\n",
        "\n",
        "        return optimizerG, optimizerD\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.args['batch_size'], shuffle=True,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.args['batch_size'], shuffle=False,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.args['batch_size'], shuffle=False,\n",
        "                          num_workers=self.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment"
      ],
      "metadata": {
        "id": "S9xr52ZczLCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io as sio"
      ],
      "metadata": {
        "id": "5tUbY5pMHa3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.Y_subspace.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWzf3LujMZJ8",
        "outputId": "db1c4e0e-64a5-45c8-9e24-c2e43b6c7937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1057, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_try = sio.loadmat('/content/klcpd_code/data/beedance/beedance-1.mat')"
      ],
      "metadata": {
        "id": "7sOofIoGc1oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_try"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGnXxC5Jc9k2",
        "outputId": "711ab309-2073-4a7e-be9f-251e549e04fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Nov  1 15:32:43 2017',\n",
              " '__version__': '1.0',\n",
              " '__globals__': [],\n",
              " 'Y': array([[0.34505421, 0.76464539, 0.54221329],\n",
              "        [0.32433136, 0.77659554, 0.47098108],\n",
              "        [0.30748942, 0.77960098, 0.4279893 ],\n",
              "        ...,\n",
              "        [0.6162483 , 0.21170825, 0.36854878],\n",
              "        [0.6050455 , 0.21422565, 0.68399638],\n",
              "        [0.62507353, 0.19284757, 0.30645727]]),\n",
              " 'L': array([[0],\n",
              "        [0],\n",
              "        [0],\n",
              "        ...,\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = data_try['Y']\n",
        "L = data_try['L']\n",
        "T, D = Y.shape\n",
        "trn_ratio = 0.6\n",
        "val_ratio=0.8\n",
        "n_trn = int(np.ceil(T * trn_ratio))\n",
        "p_wnd_dim = 25 \n",
        "trn_set_idx = range(p_wnd_dim, n_trn)\n",
        "n_val = int(np.ceil(T * val_ratio))\n",
        "\n",
        "val_set_idx = range(n_trn, n_val)\n",
        "tst_set_idx = range(n_val, T)"
      ],
      "metadata": {
        "id": "fIlcr2tReIVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trn_set_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1Anx1ETe3c8",
        "outputId": "d3a64750-97e3-436d-a089-46f3284becf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(25, 635)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = Y[trn_set_idx]\n",
        "L_train = L[trn_set_idx]\n",
        "Y_val = Y[val_set_idx]\n",
        "L_val = L[val_set_idx]\n",
        "Y_tst = Y[tst_set_idx]\n",
        "L_tst = L[tst_set_idx]"
      ],
      "metadata": {
        "id": "biBvwK1ye5Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(Y_train).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYzmYoLKgsEv",
        "outputId": "69348a78-9960-4d4d-a26d-ffb2eda5aa23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([610, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(L_train).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUWBgWbpikkL",
        "outputId": "4a4d4cb6-2451-42b2-8564-75c31524c2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([610, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L_train.reshape(-1, ).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH51K1x7guOP",
        "outputId": "20b39511-2597-4611-f414-a86675b968c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(610,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_try = TensorDataset(torch.tensor(Y_train), torch.tensor(L_train))\n",
        "Test_try = TensorDataset(torch.tensor(Y_tst), torch.tensor(L_tst))\n",
        "Val_try = TensorDataset(torch.tensor(Y_val), torch.tensor(L_val))"
      ],
      "metadata": {
        "id": "Ez0nuNMZhQlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(self, trn_ratio=0.6, val_ratio=0.8):\n",
        "    dataset = sio.loadmat(self.data_path)\n",
        "    self.Y = dataset['Y']                                   # Y: time series data, time length x number of variables\n",
        "    self.L = dataset['L']                                   # L: label of anomaly, time length x 1\n",
        "    self.T, self.D = self.Y.shape                           # T: time length; D: variable dimension\n",
        "    self.n_trn = int(np.ceil(self.T * trn_ratio))           # n_trn: first index of val set\n",
        "    self.n_val = int(np.ceil(self.T * val_ratio))           # n_val: first index of tst set\n",
        "    self.var_dim = self.D * self.sub_dim\n",
        "\n",
        "def split_data(self):\n",
        "    self.p_wnd_dim = 25\n",
        "    trn_set_idx = range(self.p_wnd_dim, self.n_trn)\n",
        "    val_set_idx = range(self.n_trn, self.n_val)\n",
        "    tst_set_idx = range(self.n_val, self.T)\n",
        "    print('n_trn ', len(trn_set_idx), 'n_val ', len(val_set_idx), 'n_tst ', len(tst_set_idx))\n",
        "    self.trn_set = self.__batchify(trn_set_idx)\n",
        "    self.val_set = self.__batchify(val_set_idx)\n",
        "    self.tst_set = self.__batchify(tst_set_idx)"
      ],
      "metadata": {
        "id": "DG0xRxgtd4SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import math\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self, args, trn_ratio=0.6, val_ratio=0.8):\n",
        "        self.cuda = args.cuda\n",
        "        self.data_path = args.data_path\n",
        "        self.p_wnd_dim = 25\n",
        "        self.f_wnd_dim = args.wnd_dim\n",
        "        self.sub_dim = args.sub_dim\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        # load data\n",
        "        self.load_data(trn_ratio=trn_ratio, val_ratio=val_ratio)\n",
        "\n",
        "        # prepare data\n",
        "        self.prepare_data()\n",
        "\n",
        "        # split data into trn/val/tst set\n",
        "        self.split_data()\n",
        "\n",
        "    # load data\n",
        "    def load_data(self, trn_ratio=0.6, val_ratio=0.8):\n",
        "        assert(os.path.lexists(self.data_path))\n",
        "        dataset = sio.loadmat(self.data_path)\n",
        "        self.Y = dataset['Y']                                   # Y: time series data, time length x number of variables\n",
        "        self.L = dataset['L']                                   # L: label of anomaly, time length x 1\n",
        "        self.T, self.D = self.Y.shape                           # T: time length; D: variable dimension\n",
        "        self.n_trn = int(np.ceil(self.T * trn_ratio))           # n_trn: first index of val set\n",
        "        self.n_val = int(np.ceil(self.T * val_ratio))           # n_val: first index of tst set\n",
        "        self.var_dim = self.D * self.sub_dim\n",
        "\n",
        "    # prepare subspace data (Hankel matrix)\n",
        "    def prepare_data(self):\n",
        "        # T x D x sub_dim\n",
        "        self.Y_subspace = np.zeros((self.T, self.D, self.sub_dim))\n",
        "        for t in range(self.sub_dim, self.T):\n",
        "            for d in range(self.D):\n",
        "                self.Y_subspace[t, d, :] = self.Y[t-self.sub_dim+1:t+1, d].flatten()\n",
        "\n",
        "        # Y_subspace is now T x (Dxsub_dim)\n",
        "        self.Y_subspace = self.Y_subspace.reshape(self.T, -1)\n",
        "\n",
        "    # split data into trn/val/tst set\n",
        "    def split_data(self):\n",
        "        trn_set_idx = range(self.p_wnd_dim, self.n_trn)\n",
        "        val_set_idx = range(self.n_trn, self.n_val)\n",
        "        tst_set_idx = range(self.n_val, self.T)\n",
        "        print('n_trn ', len(trn_set_idx), 'n_val ', len(val_set_idx), 'n_tst ', len(tst_set_idx))\n",
        "        self.trn_set = self.__batchify(trn_set_idx)\n",
        "        self.val_set = self.__batchify(val_set_idx)\n",
        "        self.tst_set = self.__batchify(tst_set_idx)\n",
        "\n",
        "    # convert augmented data in Hankel matrix to origin time series\n",
        "    # input: X_f, whose shape is batch_size x seq_len x (D*sub_dim)\n",
        "    # output: Y_t, whose shape is batch_size x D\n",
        "    def repack_data(self, X_f, batch_size):\n",
        "        Y_t = X_f[:, 0, :].contiguous().view(batch_size, self.D, self.sub_dim)\n",
        "        return Y_t[:, :, -1]\n",
        "\n",
        "    def __batchify(self, idx_set):\n",
        "        n = len(idx_set)\n",
        "        L = torch.zeros((n, 1))                             # anomaly label\n",
        "        Y = torch.zeros((n, self.D))                        # true signal\n",
        "        X_p = torch.zeros((n, self.p_wnd_dim, self.var_dim))  # past window buffer\n",
        "        X_f = torch.zeros((n, self.f_wnd_dim, self.var_dim))  # future window buffer\n",
        "\n",
        "        # XXX: dirty trick to augment the last buffer\n",
        "        data = np.concatenate((self.Y_subspace, self.Y_subspace[-self.f_wnd_dim:, :]))\n",
        "        for i in range(n):\n",
        "            l = idx_set[i] - self.p_wnd_dim\n",
        "            m = idx_set[i]\n",
        "            u = idx_set[i] + self.f_wnd_dim\n",
        "            X_p[i, :, :] = torch.from_numpy(data[l:m, :])\n",
        "            X_f[i, :, :] = torch.from_numpy(data[m:u, :])\n",
        "            Y[i, :] = torch.from_numpy(self.Y[m, :])\n",
        "            L[i] = torch.from_numpy(self.L[m])\n",
        "        return {'X_p': X_p, 'X_f': X_f, 'Y': Y, 'L': L}\n",
        "\n",
        "    def get_batches(self, data_set, batch_size, shuffle=False):\n",
        "        X_p, X_f = data_set['X_p'], data_set['X_f']\n",
        "        Y, L = data_set['Y'], data_set['L']\n",
        "        length = len(Y)\n",
        "        if shuffle:\n",
        "            index = torch.randperm(length)\n",
        "        else:\n",
        "            index = torch.LongTensor(range(length))\n",
        "        s_idx = 0\n",
        "        while (s_idx < length):\n",
        "            e_idx = min(length, s_idx + batch_size)\n",
        "            excerpt = index[s_idx:e_idx]\n",
        "            X_p_batch, X_f_batch = X_p[excerpt], X_f[excerpt]\n",
        "            Y_batch, L_batch = Y[excerpt], L[excerpt]\n",
        "            if self.cuda:\n",
        "                X_p_batch = X_p_batch.cuda()\n",
        "                X_f_batch = X_f_batch.cuda()\n",
        "                Y_batch = Y_batch.cuda()\n",
        "                L_batch = L_batch.cuda()\n",
        "\n",
        "            data = [Variable(X_p_batch),\n",
        "                    Variable(X_f_batch),\n",
        "                    Variable(Y_batch),\n",
        "                    Variable(L_batch)]\n",
        "            yield data\n",
        "            s_idx += batch_size"
      ],
      "metadata": {
        "id": "v-CF0fegwguR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1xFv4O44-am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(Data.trn_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7jdQxGe4Rqj",
        "outputId": "e4157950-a171-4214-87e3-545d736fa5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data.D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLo2M_TfKAEc",
        "outputId": "4591db87-5f59-4639-8a44-cabe76f81d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RPVTGf3HK9R",
        "outputId": "0d6449df-ccf5-4f2b-b207-9e490fafcf14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['X_p', 'X_f', 'Y', 'L'])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train['X_p'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUS2e6spHANB",
        "outputId": "7809a2f9-8a08-41b3-e164-04efcc778631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([610, 25, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train['X_f'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aZLKyv4HI0T",
        "outputId": "75583776-95a1-4250-ab3e-d4d7102a0689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([610, 10, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train['Y'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z2Npq3yHNVR",
        "outputId": "b7aa8a1f-b5b6-4a5a-f0b1-6f367e64dfe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([610, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train['L'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXGG_LwUHQqo",
        "outputId": "2f7c9c77-4d62-41a9-9a1c-0fe6f1ac5730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([610, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Test['L'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyFDBqeqHTUr",
        "outputId": "ef57e64d-8e1b-437d-a01b-90380f3c9724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([211, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvbQ7nqcHqsh",
        "outputId": "b7109ae4-8080-4590-8296-c3e8d8fea503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_path': '//content/klcpd_code/data/beedance/beedance-1.mat',\n",
              " 'trn_ratio': 0.6,\n",
              " 'val_ratio': 0.8,\n",
              " 'gpu': 0,\n",
              " 'cuda': True,\n",
              " 'random_seed': 1126,\n",
              " 'wnd_dim': 10,\n",
              " 'sub_dim': 1,\n",
              " 'RNN_hid_dim': 10,\n",
              " 'batch_size': 128,\n",
              " 'max_iter': 100,\n",
              " 'optim': 'adam',\n",
              " 'lr': 0.0003,\n",
              " 'weight_decay': 0.0,\n",
              " 'momentum': 0.0,\n",
              " 'grad_clip': 10.0,\n",
              " 'eval_freq': 50,\n",
              " 'CRITIC_ITERS': 5,\n",
              " 'weight_clip': 0.1,\n",
              " 'lambda_ae': 0.001,\n",
              " 'lambda_real': 0.1,\n",
              " 'save_path': '/content/exp_simulate/jumpingmean/save_RNN'}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataroot = '/content/klcpd_code/data'\n",
        "dataset = 'beedance'\n",
        "gpu = 0\n",
        "wnd_dim_list = [5, 10, 15, 20, 25, 30]\n",
        "lambda_ae = 1e-3\n",
        "lambda_real = 1\n",
        "max_iter = 2000\n",
        "batch_size = 64\n",
        "eval_freq = 25\n",
        "weight_clip = .1\n",
        "#parser.add_argument('--save_dir', type=str, default='experiment_log', help='experiment directory for saving train log and models')\n",
        "save_dir = 'experiment_log'"
      ],
      "metadata": {
        "id": "4lBm6al-qwNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_klcpd_exp(dataset):\n",
        "    for wnd_dim in wnd_dim_list:\n",
        "        data_dir = os.path.join(dataroot, dataset)\n",
        "        for data_path in glob.glob('%s/*.mat' % (data_dir)):\n",
        "            data_name = data_path.split('/')[-1].split('.')[0]\n",
        "            save_name = '%s.wnd-%d.lambda_ae-%f.lambda_real-%f.clip-%f' % (data_name, wnd_dim, lambda_ae, lambda_real, weight_clip)\n",
        "            #save_path = '%s/%s' % (args.save_dir, save_name)\n",
        "            save_path = '%s/%s' % (save_dir, save_name)\n",
        "            trn_log_path = '%s.trn.log' % (save_path)\n",
        "            option = '--data_path %s --wnd_dim %d --lambda_ae %f --lambda_real %f --weight_clip %f --max_iter %d --batch_size %d --eval_freq %d --save_path %s' \\\n",
        "                % (data_path, wnd_dim, lambda_ae, lambda_real, weight_clip, max_iter, batch_size, eval_freq, save_path)\n",
        "            cmd = 'CUDA_VISIBLE_DEVICES=%d python -u klcpd.py %s 2>&1 | tee %s' % (gpu, option, trn_log_path)\n",
        "            #print(cmd)\n",
        "            os.system(cmd)"
      ],
      "metadata": {
        "id": "Q6tARTS3s6TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize parameters"
      ],
      "metadata": {
        "id": "SxMsyQvC1Ada"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def median_heuristic(X, beta=0.5):\n",
        "    max_n = min(30000, X.shape[0])\n",
        "    D2 = euclidean_distances(X[:max_n], squared=True)\n",
        "    med_sqdist = np.median(D2[np.triu_indices_from(D2, k=1)])\n",
        "    beta_list = [beta**2, beta**1, 1, (1.0/beta)**1, (1.0/beta)**2]\n",
        "    return [med_sqdist * b for b in beta_list]\n"
      ],
      "metadata": {
        "id": "R8uMWxcm5F-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========= Setup input argument =========#\n",
        "parser = argparse.ArgumentParser(description='PyTorch Time series forecasting')\n",
        "#parser.add_argument('--data_path', type=str, required=True, help='path to data in matlab format')\n",
        "parser.add_argument('--data_path', type=str, default = '//content/klcpd_code/data/beedance/beedance-1.mat', help='path to data in matlab format')\n",
        "parser.add_argument('--trn_ratio', type=float, default=0.6,help='how much data used for training')\n",
        "parser.add_argument('--val_ratio', type=float, default=0.8,help='how much data used for validation')\n",
        "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
        "parser.add_argument('--cuda', type=str, default=True, help='use gpu or not')\n",
        "parser.add_argument('--random_seed', type=int, default=1126,help='random seed')\n",
        "#parser.add_argument('--wnd_dim', type=int, required=True, default=10, help='window size (past and future)')\n",
        "parser.add_argument('--wnd_dim', type=int, default=10, help='window size (past and future)')\n",
        "parser.add_argument('--sub_dim', type=int, default=1, help='dimension of subspace embedding')\n",
        "\n",
        "# RNN hyperparemters\n",
        "parser.add_argument('--RNN_hid_dim', type=int, default=10, help='number of RNN hidden units')\n",
        "\n",
        "# optimization\n",
        "parser.add_argument('--batch_size', type=int, default=128, help='batch size for training')\n",
        "parser.add_argument('--max_iter', type=int, default=100, help='max iteration for pretraining RNN')\n",
        "parser.add_argument('--optim', type=str, default='adam', help='sgd|rmsprop|adam for optimization method')\n",
        "parser.add_argument('--lr', type=float, default=3e-4, help='learning rate')\n",
        "parser.add_argument('--weight_decay', type=float, default=0., help='weight decay (L2 regularization)')\n",
        "parser.add_argument('--momentum', type=float, default=0.0, help='momentum for sgd')\n",
        "parser.add_argument('--grad_clip', type=float, default=10.0, help='gradient clipping for RNN (both netG and netD)')\n",
        "parser.add_argument('--eval_freq', type=int, default=50, help='evaluation frequency per generator update')\n",
        "\n",
        "# GAN\n",
        "parser.add_argument('--CRITIC_ITERS', type=int, default=5, help='number of updates for critic per generator')\n",
        "parser.add_argument('--weight_clip', type=float, default=.1, help='weight clipping for crtic')\n",
        "parser.add_argument('--lambda_ae', type=float, default=0.001, help='coefficient for the reconstruction loss')\n",
        "parser.add_argument('--lambda_real', type=float, default=0.1, help='coefficient for the real MMD2 loss')\n",
        "\n",
        "# save models  /content/exp_simulate/jumpingmean/save_RNN\n",
        "parser.add_argument('--save_path', type=str,  default='/content/exp_simulate/jumpingmean/save_RNN',help='path to save the final model')\n",
        "\n",
        "args, unknown = parser.parse_known_args()"
      ],
      "metadata": {
        "id": "LitvT5Yrwu75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir 'exp_simulate'\n",
        "!cd '/content/exp_simulate'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLj4rYjDHsXw",
        "outputId": "1ffc3674-7033-4391-fd0a-8911af35c411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘exp_simulate’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/exp_simulate\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtIqTIF6H83y",
        "outputId": "110d7bc1-cbe8-4c32-aeb9-c67404822c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(args.save_path):\n",
        "    os.mkdir(args.save_path)\n",
        "assert(os.path.isdir(args.save_path))\n",
        "# assert(args.sub_dim == 1)\n",
        "\n",
        "#XXX For Yahoo dataset, trn_ratio=0.50, val_ratio=0.75\n",
        "if 'yahoo' in args.data_path:\n",
        "    args.trn_ratio = 0.50\n",
        "    args.val_ratio = 0.75"
      ],
      "metadata": {
        "id": "r6ue7l0ExWUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = vars(args)"
      ],
      "metadata": {
        "id": "_MhbQImH1UrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(args.save_path):\n",
        "    os.mkdir(args.save_path)\n",
        "assert(os.path.isdir(args.save_path))\n",
        "# assert(args.sub_dim == 1)\n",
        "\n",
        "#XXX For Yahoo dataset, trn_ratio=0.50, val_ratio=0.75\n",
        "if 'yahoo' in args.data_path:\n",
        "    args.trn_ratio = 0.50\n",
        "    args.val_ratio = 0.75\n",
        "\n",
        "\n",
        "\n",
        "# ========= Setup GPU device and fix random seed=========#\n",
        "if torch.cuda.is_available():\n",
        "    args.cuda = True\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    print('Using GPU device', torch.cuda.current_device())\n",
        "else:\n",
        "    raise EnvironmentError(\"GPU device not available!\")\n",
        "np.random.seed(seed=args.random_seed)\n",
        "random.seed(args.random_seed)\n",
        "torch.manual_seed(args.random_seed)\n",
        "torch.cuda.manual_seed(args.random_seed)\n",
        "# [INFO] cudnn.benckmark=True enable cudnn auto-tuner to find the best algorithm to use for your hardware\n",
        "# [INFO] benchmark mode is good whenever input sizes of network do not vary much!!!\n",
        "# [INFO] https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
        "# [INFO] https://discuss.pytorch.org/t/pytorch-performance/3079/2\n",
        "torch.backends.cudnn.benchmark == True\n",
        "\n",
        "# [INFO} For reproducibility and debugging, set cudnn.enabled=False\n",
        "# [INFO] Some operations are non-deterministic when cudnn.enabled=True\n",
        "# [INFO] https://discuss.pytorch.org/t/non-determinisic-results/459\n",
        "# [INFO] https://discuss.pytorch.org/t/non-reproducible-result-with-gpu/1831\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "# ========= Load Dataset and initialize model=========#\n",
        "Data = DataLoader(args, trn_ratio=args.trn_ratio, val_ratio=args.val_ratio)\n",
        "netG = NetG(args, Data)\n",
        "netD = NetD(args, Data)"
      ],
      "metadata": {
        "id": "uofcJmRcx_ja",
        "outputId": "4cb6ac21-3c8a-4601-ac2a-3d430f8adbc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU device 0\n",
            "n_trn  610 n_val  211 n_tst  211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.cuda:\n",
        "    netG.cuda()\n",
        "    netD.cuda()\n",
        "netG_params_count = sum([p.nelement() for p in netG.parameters()])\n",
        "netD_params_count = sum([p.nelement() for p in netD.parameters()])\n",
        "print(netG)\n",
        "print(netD)\n",
        "print('netG has number of parameters: %d' % (netG_params_count))\n",
        "print('netD has number of parameters: %d' % (netD_params_count))\n",
        "one = torch.cuda.FloatTensor([1])\n",
        "one = torch.tensor(1, dtype=torch.float).cuda()\n",
        "mone = one * -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTlwAtLv2a0S",
        "outputId": "f3dee9b3-a3e8-46a8-941d-661b40f8e59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetG(\n",
            "  (rnn_enc_layer): GRU(3, 10, batch_first=True)\n",
            "  (rnn_dec_layer): GRU(3, 10, batch_first=True)\n",
            "  (fc_layer): Linear(in_features=10, out_features=3, bias=True)\n",
            ")\n",
            "NetD(\n",
            "  (rnn_enc_layer): GRU(3, 10, batch_first=True)\n",
            "  (rnn_dec_layer): GRU(10, 3, batch_first=True)\n",
            ")\n",
            "netG has number of parameters: 933\n",
            "netD has number of parameters: 585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one = torch.tensor(1, dtype=torch.float).cuda()\n",
        "one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGZ3rTt6KKxr",
        "outputId": "cc63428d-fa9c-470b-9040-604678be46a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcg_LEmd6edA",
        "outputId": "1970c8c0-2e17-4480-d052-665336bd992f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizers"
      ],
      "metadata": {
        "id": "SkFwR7PwaBZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class Optim(object):\n",
        "\n",
        "    def _makeOptimizer(self):\n",
        "        if self.method == 'sgd':\n",
        "            self.optimizer = optim.SGD(self.params, lr=self.lr, weight_decay=self.weight_decay, momentum=self.momentum)\n",
        "        elif self.method == 'adagrad':\n",
        "            self.optimizer = optim.Adagrad(self.params, lr=self.lr, weight_decay=self.weight_decay)\n",
        "        elif self.method == 'rmsprop':\n",
        "            self.optimizer = optim.RMSprop(self.params, lr=self.lr, weight_decay=self.weight_decay, momentum=self.momentum)\n",
        "        elif self.method == 'adam':\n",
        "            self.optimizer = optim.Adam(self.params, lr=self.lr, weight_decay=self.weight_decay)\n",
        "        else:\n",
        "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
        "\n",
        "    def __init__(self, params, method, lr=0.1, grad_clip=10.0, weight_decay=0., momentum=0.9):\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.max_norm = grad_clip\n",
        "        self.weight_decay = weight_decay\n",
        "        self.momentum = momentum\n",
        "        self.method = method\n",
        "        self._makeOptimizer()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad();\n",
        "\n",
        "    def step(self):\n",
        "        # Compute gradients norm.\n",
        "        total_norm = 0\n",
        "        for p in self.params:\n",
        "            total_norm += p.grad.data.norm(2) ** 2\n",
        "        total_norm = total_norm ** (1. / 2)\n",
        "        clip_coef = self.max_norm / (total_norm + 1e-6)\n",
        "\n",
        "        # grading clipping\n",
        "        if clip_coef < 1:\n",
        "            for p in self.params:\n",
        "                p.grad.data.mul_(clip_coef)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    # decay learning rate if val perf does not improve or we hit the start_decay_at limit\n",
        "    def updateLearningRate(self, ppl, epoch):\n",
        "        if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
        "            self.start_decay = True\n",
        "        if self.last_ppl is not None and ppl > self.last_ppl:\n",
        "            self.start_decay = True\n",
        "\n",
        "        if self.start_decay:\n",
        "            self.lr = self.lr * self.lr_decay\n",
        "            print(\"Decaying learning rate to %g\" % self.lr)\n",
        "        #only decay for one epoch\n",
        "        self.start_decay = False\n",
        "\n",
        "        self.last_ppl = ppl\n",
        "        self._makeOptimizer()\n",
        "\n",
        "\n",
        "        # ========= Setup loss function and optimizer  =========#\n",
        "optimizerG = Optim(netG.parameters(),\n",
        "                   args.optim,\n",
        "                   lr=args.lr,\n",
        "                   grad_clip=args.grad_clip,\n",
        "                   weight_decay=args.weight_decay,\n",
        "                   momentum=args.momentum)\n",
        "\n",
        "optimizerD = Optim(netD.parameters(),\n",
        "                   args.optim,\n",
        "                   lr=args.lr,\n",
        "                   grad_clip=args.grad_clip,\n",
        "                   weight_decay=args.weight_decay,\n",
        "                   momentum=args.momentum)\n"
      ],
      "metadata": {
        "id": "eIoQ6X7X3yGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Inquiry"
      ],
      "metadata": {
        "id": "zaSLsy0WZitF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train = Data.trn_set"
      ],
      "metadata": {
        "id": "bJte_0Hf6AMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test = Data.tst_set"
      ],
      "metadata": {
        "id": "CTeNcNLV6GBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "sze3XP976EKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya9AYHbBRk-8",
        "outputId": "ec1c244a-be3d-4775-c58b-fbb063f6381e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_path': '//content/klcpd_code/data/beedance/beedance-1.mat',\n",
              " 'trn_ratio': 0.6,\n",
              " 'val_ratio': 0.8,\n",
              " 'gpu': 0,\n",
              " 'cuda': True,\n",
              " 'random_seed': 1126,\n",
              " 'wnd_dim': 10,\n",
              " 'sub_dim': 1,\n",
              " 'RNN_hid_dim': 10,\n",
              " 'batch_size': 128,\n",
              " 'max_iter': 100,\n",
              " 'optim': 'adam',\n",
              " 'lr': 0.0003,\n",
              " 'weight_decay': 0.0,\n",
              " 'momentum': 0.0,\n",
              " 'grad_clip': 10.0,\n",
              " 'eval_freq': 50,\n",
              " 'CRITIC_ITERS': 5,\n",
              " 'weight_clip': 0.1,\n",
              " 'lambda_ae': 0.001,\n",
              " 'lambda_real': 0.1,\n",
              " 'save_path': '/content/exp_simulate/jumpingmean/save_RNN',\n",
              " 'sqdist': 0.15892872017303572}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = KLCPD(netG, netD, d, Train_try, Test_try)\n",
        "trainer = pl.Trainer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "akoq_JW1TqJu",
        "outputId": "3ddf7e8b-5443-4ea9-c678-e691522d7e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-3c097ec41830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKLCPD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_try\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_try\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1dc438ea7edb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, netG, netD, args, train_dataset, test_dataset, num_workers)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msigma_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedian_heuristic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sqdist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-14add1fb9091>\u001b[0m in \u001b[0;36mmedian_heuristic\u001b[0;34m(X, beta)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmedian_heuristic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmax_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mD2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmed_sqdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu_indices_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "trainer.fit(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588,
          "referenced_widgets": [
            "07ba8ed1f17e40a0b6e5167ce443bf6e",
            "3c7e65dff5a54b359c91c34af2d4e357",
            "6eaf5861d35a40b8a626835b42dcbe9f",
            "2b90cc7c349041cb8b622beae56cd708",
            "c4d6528e14524d2a981b0efa9deddc56",
            "2c9478111e6d4645b2981cbf25f5f7f0",
            "4251dedeb1fe4ffbb3e686b069543bcf",
            "ad5eff1efd404aefbacf525e26ff0cc8",
            "fce7c65521f347f0a675c43b5c3f8465",
            "b5a914f4d5d043e1b98f4987dcc4d479",
            "25bb2aceb4d24a82b27fac846da0c046"
          ]
        },
        "id": "lxIb_c76TnxH",
        "outputId": "ddee89e1-e4b4-4716-dbe2-91793d53bd79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
            "  rank_zero_warn(\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name | Type | Params\n",
            "------------------------------\n",
            "0 | netG | NetG | 933   \n",
            "1 | netD | NetD | 585   \n",
            "------------------------------\n",
            "1.5 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.5 K     Total params\n",
            "0.006     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07ba8ed1f17e40a0b6e5167ce443bf6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-da65be8db77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"`Trainer.fit()` requires a `LightningModule`, got: {model.__class__.__qualname__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         )\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m                 \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataloader_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_step\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-1dc438ea7edb>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mX_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_history_future_separation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wnd_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mX_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wnd_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-1dc438ea7edb>\u001b[0m in \u001b[0;36m_history_future_separation\u001b[0;34m(data, window)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# separation for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_history_future_separation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CnLxPYqa2rvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C67d_7if31A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.configure_optimizers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l5jQZdSTBi2",
        "outputId": "b26e5bec-1def-4de1-b206-0fc3311bf3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: False\n",
              "     lr: 0.0003\n",
              "     maximize: False\n",
              "     weight_decay: 0.0\n",
              " ), Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: False\n",
              "     lr: 0.0003\n",
              "     maximize: False\n",
              "     weight_decay: 0.0\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.train_dataloader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02W3OQl6S-po",
        "outputId": "593d8256-143f-40ed-c691-53625fd1b105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fa447876bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhrbQbW34OK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pm7y2q2Y4OOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LsrP--Ri4OSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def median_heuristic(X, beta=0.5):\n",
        "    max_n = min(30000, X.shape[0])\n",
        "    D2 = euclidean_distances(X[:max_n], squared=True)\n",
        "    med_sqdist = np.median(D2[np.triu_indices_from(D2, k=1)])\n",
        "    beta_list = [beta**2, beta**1, 1, (1.0/beta)**1, (1.0/beta)**2]\n",
        "    return [med_sqdist * b for b in beta_list]"
      ],
      "metadata": {
        "id": "AbP6sXha4OWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sigma for mixture of RBF kernel in MMD\n",
        "#sigma_list = [1.0]\n",
        "#sigma_list = mmd_util.median_heuristic(Data.Y_subspace, beta=1.)\n",
        "sigma_list = median_heuristic(Data.Y_subspace, beta=.5)\n",
        "sigma_var = torch.FloatTensor(sigma_list).cuda()\n",
        "print('sigma_list:', sigma_var)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGO26VOb4OaK",
        "outputId": "1e18b76b-e981-4a6f-870e-67315c45f9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigma_list: tensor([0.0397, 0.0795, 0.1589, 0.3179, 0.6357], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========= Main loop for adversarial training kernel with negative samples X_f + noise =========#\n",
        "Y_val = Data.val_set['Y'].numpy()\n",
        "L_val = Data.val_set['L'].numpy()\n",
        "Y_tst = Data.tst_set['Y'].numpy()\n",
        "L_tst = Data.tst_set['L'].numpy()\n",
        "\n",
        "n_batchs = int(math.ceil(len(Data.trn_set['Y']) / float(args.batch_size)))\n",
        "print('n_batchs', n_batchs, 'batch_size', args.batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIDhO2uD5im0",
        "outputId": "a27bfebe-94fd-4a51-81f8-20034dc798b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_batchs 5 batch_size 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.max_iter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJyZIEhU7XBA",
        "outputId": "dd9e37a6-6dfc-42a0-8501-18d6017046ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_batchs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g1pVqP77aeT",
        "outputId": "0e6fb5f9-a673-4ef4-9def-fdf57fceaf00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.CRITIC_ITERS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpRHyaOR7eku",
        "outputId": "888e6ec6-3a0a-4adf-ddc6-9eaafbaf0c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bidx == n_batchs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1BNiCDK7iZB",
        "outputId": "ca3fdbea-08fb-42e7-8b63-90f8f39d824b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bidx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_FaG-gb7rob",
        "outputId": "9b8947db-a3cb-4b85-bb99-76ca06e4e089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        trn_loader = Data.get_batches(Data.trn_set, batch_size=args.batch_size, shuffle=True)\n",
        "        bidx = 0\n",
        "        print(bidx)\n",
        "        ############################\n",
        "        # (1) Update D network\n",
        "        ############################\n",
        "        for p in netD.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        for diters in range(args.CRITIC_ITERS):\n",
        "            # clamp parameters of NetD encoder to a cube\n",
        "            for p in netD.rnn_enc_layer.parameters():\n",
        "                p.data.clamp_(-args.weight_clip, args.weight_clip)\n",
        "            if bidx == n_batchs:\n",
        "                break\n",
        "\n",
        "            inputs = next(trn_loader)\n",
        "            X_p, X_f, Y_true = inputs[0], inputs[1], inputs[2]\n",
        "            batch_size = X_p.size(0)\n",
        "            bidx += 1\n",
        "\n",
        "            # real data\n",
        "            X_p_enc, X_p_dec = netD(X_p)\n",
        "            X_f_enc, X_f_dec = netD(X_f)\n",
        "\n",
        "            # fake data\n",
        "            noise = torch.cuda.FloatTensor(1, batch_size, args.RNN_hid_dim).normal_(0, 1)\n",
        "            noise = Variable(noise, volatile=True) # total freeze netG\n",
        "            Y_f = Variable(netG(X_p, X_f, noise).data)\n",
        "            Y_f_enc, Y_f_dec = netD(Y_f)\n",
        "\n",
        "            # batchwise MMD2 loss between X_f and Y_f\n",
        "            D_mmd2 = batch_mmd2_loss(X_f_enc, Y_f_enc, sigma_var)\n",
        "\n",
        "            # batchwise MMD loss between X_p and X_f\n",
        "            mmd2_real = batch_mmd2_loss(X_p_enc, X_f_enc, sigma_var)\n",
        "\n",
        "            # reconstruction loss\n",
        "            real_L2_loss = torch.mean((X_f - X_f_dec)**2)\n",
        "            #real_L2_loss = torch.mean((X_p - X_p_dec)**2)\n",
        "            fake_L2_loss = torch.mean((Y_f - Y_f_dec)**2)\n",
        "            #fake_L2_loss = torch.mean((Y_f - Y_f_dec)**2) * 0.0\n",
        "\n",
        "            # update netD\n",
        "            netD.zero_grad()\n",
        "            lossD = D_mmd2.mean() - lambda_ae * (real_L2_loss + fake_L2_loss) - lambda_real * mmd2_real.mean()\n",
        "            #lossD = 0.0 * D_mmd2.mean() - lambda_ae * (real_L2_loss + fake_L2_loss) - lambda_real * mmd2_real.mean()\n",
        "            #lossD = -real_L2_loss\n",
        "            #lossD.backward(mone)\n",
        "            lossD.backward(mone)\n",
        "            optimizerD.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmVYagdA9MQB",
        "outputId": "79c6ac9b-5388-46c2-d971-5deba237675b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-1e47c6509d7c>:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  noise = Variable(noise, volatile=True) # total freeze netG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQgMOvEeAdnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_mmd2_loss(X_p_enc, X_f_enc, sigma_var).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS148_0L-Z0W",
        "outputId": "8af43b8c-8043-4fdb-cff2-3758984aa738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([98])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "real_L2_loss.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzdqhY-2-njA",
        "outputId": "8f159ee3-8a31-43bb-dd1b-bcb733273ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_ae * (real_L2_loss + fake_L2_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHnvdDVS-k6c",
        "outputId": "056d1533-f9e1-4738-f449-dcf533a2b70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0003, device='cuda:0', grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_f_enc.shape, Y_f_enc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuZJtfHB-Ou1",
        "outputId": "24cf6d51-e2b6-489c-c684-b0ff2d171c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([98, 10, 10]), torch.Size([98, 10, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjMkmu8i9c1g",
        "outputId": "9882cf10-9db8-4a7d-c004-47222144b40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.7536, device='cuda:0', grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv5RjpdE9fd1",
        "outputId": "97fe83e2-9f59-4496-9696-eee3d069bc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "lambda_ae = args.lambda_ae\n",
        "lambda_real = args.lambda_real\n",
        "gen_iterations = 0\n",
        "total_time = 0.\n",
        "best_epoch = -1\n",
        "best_val_mae = 1e+6\n",
        "best_val_auc = -1\n",
        "best_tst_auc = -1\n",
        "best_mmd_real = 1e+6\n",
        "start_time = time.time()\n",
        "print('start training: lambda_ae', lambda_ae, 'lambda_real', lambda_real, 'weight_clip', args.weight_clip)\n",
        "for epoch in range(1, args.max_iter + 1):\n",
        "    trn_loader = Data.get_batches(Data.trn_set, batch_size=args.batch_size, shuffle=True)\n",
        "    bidx = 0\n",
        "    while bidx < n_batchs:\n",
        "        #print(bidx)\n",
        "        ############################\n",
        "        # (1) Update D network\n",
        "        ############################\n",
        "        for p in netD.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        for diters in range(args.CRITIC_ITERS):\n",
        "            # clamp parameters of NetD encoder to a cube\n",
        "            for p in netD.rnn_enc_layer.parameters():\n",
        "                p.data.clamp_(-args.weight_clip, args.weight_clip)\n",
        "            if bidx == n_batchs:\n",
        "                break\n",
        "\n",
        "            inputs = next(trn_loader)\n",
        "            X_p, X_f, Y_true = inputs[0], inputs[1], inputs[2]\n",
        "            batch_size = X_p.size(0)\n",
        "            bidx += 1\n",
        "\n",
        "            # real data\n",
        "            X_p_enc, X_p_dec = netD(X_p)\n",
        "            X_f_enc, X_f_dec = netD(X_f)\n",
        "\n",
        "            # fake data\n",
        "            noise = torch.cuda.FloatTensor(1, batch_size, args.RNN_hid_dim).normal_(0, 1)\n",
        "            noise = Variable(noise, volatile=True) # total freeze netG\n",
        "            Y_f = Variable(netG(X_p, X_f, noise).data)\n",
        "            Y_f_enc, Y_f_dec = netD(Y_f)\n",
        "\n",
        "            # batchwise MMD2 loss between X_f and Y_f\n",
        "            D_mmd2 = batch_mmd2_loss(X_f_enc, Y_f_enc, sigma_var)\n",
        "\n",
        "            # batchwise MMD loss between X_p and X_f\n",
        "            mmd2_real = batch_mmd2_loss(X_p_enc, X_f_enc, sigma_var)\n",
        "\n",
        "            # reconstruction loss\n",
        "            real_L2_loss = torch.mean((X_f - X_f_dec)**2)\n",
        "            #real_L2_loss = torch.mean((X_p - X_p_dec)**2)\n",
        "            fake_L2_loss = torch.mean((Y_f - Y_f_dec)**2)\n",
        "            #fake_L2_loss = torch.mean((Y_f - Y_f_dec)**2) * 0.0\n",
        "\n",
        "            # update netD\n",
        "            netD.zero_grad()\n",
        "            lossD = D_mmd2.mean() - lambda_ae * (real_L2_loss + fake_L2_loss) - lambda_real * mmd2_real.mean()\n",
        "            #lossD = 0.0 * D_mmd2.mean() - lambda_ae * (real_L2_loss + fake_L2_loss) - lambda_real * mmd2_real.mean()\n",
        "            #lossD = -real_L2_loss\n",
        "            #lossD.backward(mone)\n",
        "            lossD.backward(mone)\n",
        "            optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network\n",
        "        ############################\n",
        "        for p in netD.parameters():\n",
        "            p.requires_grad = False  # to avoid computation\n",
        "\n",
        "        if bidx == n_batchs:\n",
        "            break\n",
        "\n",
        "        inputs = next(trn_loader)\n",
        "        X_p, X_f = inputs[0], inputs[1]\n",
        "        batch_size = X_p.size(0)\n",
        "        bidx += 1\n",
        "\n",
        "        # real data\n",
        "        X_f_enc, X_f_dec = netD(X_f)\n",
        "\n",
        "        # fake data\n",
        "        noise = torch.cuda.FloatTensor(1, batch_size, args.RNN_hid_dim).normal_(0, 1)\n",
        "        noise = Variable(noise)\n",
        "        Y_f = netG(X_p, X_f, noise)\n",
        "        Y_f_enc, Y_f_dec = netD(Y_f)\n",
        "\n",
        "        # batchwise MMD2 loss between X_f and Y_f\n",
        "        G_mmd2 = batch_mmd2_loss(X_f_enc, Y_f_enc, sigma_var)\n",
        "\n",
        "        # update netG\n",
        "        netG.zero_grad()\n",
        "        lossG = G_mmd2.mean()\n",
        "        #lossG = 0.0 * G_mmd2.mean()\n",
        "        lossG.backward(one)\n",
        "        optimizerG.step()\n",
        "\n",
        "        #G_mmd2 = Variable(torch.FloatTensor(batch_size).zero_())\n",
        "        gen_iterations += 1\n",
        "\n",
        "        print('[%5d/%5d] [%5d/%5d] [%6d] D_mmd2 %.4e G_mmd2 %.4e mmd2_real %.4e real_L2 %.6f fake_L2 %.6f'\n",
        "              % (epoch, args.max_iter, bidx, n_batchs, gen_iterations,\n",
        "                 D_mmd2.mean().data[0], G_mmd2.mean().data[0], mmd2_real.mean().data[0],\n",
        "                 real_L2_loss.data[0], fake_L2_loss.data[0]))\n",
        "\n",
        "        if gen_iterations % args.eval_freq == 0:\n",
        "            # ========= Main block for evaluate MMD(X_p_enc, X_f_enc) on RNN codespace  =========#\n",
        "            val_dict = valid_epoch(Data, Data.val_set, netD, args.batch_size, Y_val, L_val)\n",
        "            tst_dict = valid_epoch(Data, Data.tst_set, netD, args.batch_size, Y_tst, L_tst)\n",
        "            total_time = time.time() - start_time\n",
        "            print('iter %4d tm %4.2fm val_mse %.1f val_mae %.1f val_auc %.6f'\n",
        "                    % (epoch, total_time / 60.0, val_dict['mse'], val_dict['mae'], val_dict['auc']), end='')\n",
        "\n",
        "            print (\" tst_mse %.1f tst_mae %.1f tst_auc %.6f\" % (tst_dict['mse'], tst_dict['mae'], tst_dict['auc']), end='')\n",
        "\n",
        "            assert(np.isnan(val_dict['auc']) != True)\n",
        "            #if val_dict['auc'] > best_val_auc:\n",
        "            #if val_dict['auc'] > best_val_auc and mmd2_real.mean().data[0] < best_mmd_real:\n",
        "            if mmd2_real.mean().data[0] < best_mmd_real:\n",
        "                best_mmd_real = mmd2_real.mean().data[0]\n",
        "                best_val_mae = val_dict['mae']\n",
        "                best_val_auc = val_dict['auc']\n",
        "                best_tst_auc = tst_dict['auc']\n",
        "                best_epoch = epoch\n",
        "                save_pred_name = '%s/pred.pkl' % (args.save_path)\n",
        "                with open(save_pred_name, 'wb') as f:\n",
        "                    pickle.dump(tst_dict, f)\n",
        "                torch.save(netG.state_dict(), '%s/netG.pkl' % (args.save_path))\n",
        "                torch.save(netD.state_dict(), '%s/netD.pkl' % (args.save_path))\n",
        "            print(\" [best_val_auc %.6f best_tst_auc %.6f best_epoch %3d]\" % (best_val_auc, best_tst_auc, best_epoch))\n",
        "\n",
        "        # stopping condition\n",
        "        #if best_mmd_real < 1e-4:\n",
        "        if mmd2_real.mean().data[0] < 1e-5:\n",
        "            exit(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37OVoywl4Od9",
        "outputId": "c19317f0-add9-4604-8da7-c11b28b07562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training: lambda_ae 0.001 lambda_real 0.1 weight_clip 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-81-f6d3bbf23736>:44: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  noise = Variable(noise, volatile=True) # total freeze netG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" [best_val_auc %.6f best_tst_auc %.6f best_epoch %3d]\" % (best_val_auc, best_tst_auc, best_epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnlzHslG7JHf",
        "outputId": "899bd835-7df0-42dd-e0ed-88e49a050538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [best_val_auc -1.000000 best_tst_auc -1.000000 best_epoch  -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CcEiOnhaL0Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XV8nGfSBL0ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def median_heuristic(X, beta=0.5):\n",
        "    max_n = min(30000, X.shape[0])\n",
        "    D2 = euclidean_distances(X[:max_n], squared=True)\n",
        "    med_sqdist = np.median(D2[np.triu_indices_from(D2, k=1)])\n",
        "    beta_list = [beta**2, beta**1, 1, (1.0/beta)**1, (1.0/beta)**2]\n",
        "    return [med_sqdist * b for b in beta_list]\n",
        "\n",
        "\n",
        "# X_p_enc: batch_size x seq_len x hid_dim\n",
        "# X_f_enc: batch_size x seq_len x hid_dim\n",
        "# hid_dim could be either dataspace_dim or codespace_dim\n",
        "# return: MMD2(X_p_enc[i,:,:], X_f_enc[i,:,:]) for i = 1:batch_size\n",
        "def batch_mmd2_loss(X_p_enc, X_f_enc, sigma_var):\n",
        "    # some constants\n",
        "    n_basis = 1024\n",
        "    gumbel_lmd = 1e+6\n",
        "    cnst = math.sqrt(1. / n_basis)\n",
        "    n_mixtures = sigma_var.size(0)\n",
        "    n_samples = n_basis * n_mixtures\n",
        "    batch_size, seq_len, nz = X_p_enc.size()\n",
        "\n",
        "    # gumbel trick to get masking matrix to uniformly sample sigma\n",
        "    # input: (batch_size*n_samples, nz)\n",
        "    # output: (batch_size, n_samples, nz)\n",
        "    def sample_gmm(W, batch_size):\n",
        "        U = torch.cuda.FloatTensor(batch_size*n_samples, n_mixtures).uniform_()\n",
        "        sigma_samples = F.softmax(U * gumbel_lmd).matmul(sigma_var)\n",
        "        W_gmm = W.mul(1. / sigma_samples.unsqueeze(1))\n",
        "        W_gmm = W_gmm.view(batch_size, n_samples, nz)\n",
        "        return W_gmm\n",
        "\n",
        "    W = Variable(torch.cuda.FloatTensor(batch_size*n_samples, nz).normal_(0, 1))\n",
        "    W_gmm = sample_gmm(W, batch_size)                                   # batch_size x n_samples x nz\n",
        "    W_gmm = torch.transpose(W_gmm, 1, 2).contiguous()                   # batch_size x nz x n_samples\n",
        "    XW_p = torch.bmm(X_p_enc, W_gmm)                                    # batch_size x seq_len x n_samples\n",
        "    XW_f = torch.bmm(X_f_enc, W_gmm)                                    # batch_size x seq_len x n_samples\n",
        "    z_XW_p = cnst * torch.cat((torch.cos(XW_p), torch.sin(XW_p)), 2)\n",
        "    z_XW_f = cnst * torch.cat((torch.cos(XW_f), torch.sin(XW_f)), 2)\n",
        "    batch_mmd2_rff = torch.sum((z_XW_p.mean(1) - z_XW_f.mean(1))**2, 1)\n",
        "    return batch_mmd2_rff"
      ],
      "metadata": {
        "id": "lKxlyWLzL01Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Main loop for adversarial training kernel with negative samples X_f + noise =========#\n",
        "Y_val = Data.val_set['Y'].numpy()\n",
        "L_val = Data.val_set['L'].numpy()\n",
        "Y_tst = Data.tst_set['Y'].numpy()\n",
        "L_tst = Data.tst_set['L'].numpy()\n",
        "\n",
        "n_batchs = int(math.ceil(len(Data.trn_set['Y']) / float(args.batch_size)))\n",
        "print('n_batchs', n_batchs, 'batch_size', args.batch_size)\n",
        "\n",
        "lambda_ae = args.lambda_ae\n",
        "lambda_real = args.lambda_real\n",
        "gen_iterations = 0\n",
        "total_time = 0.\n",
        "best_epoch = -1\n",
        "best_val_mae = 1e+6\n",
        "best_val_auc = -1\n",
        "best_tst_auc = -1\n",
        "best_mmd_real = 1e+6\n",
        "start_time = time.time()\n",
        "print('start training: lambda_ae', lambda_ae, 'lambda_real', lambda_real, 'weight_clip', args.weight_clip)\n",
        "for epoch in range(1, args.max_iter + 1):\n",
        "    trn_loader = Data.get_batches(Data.trn_set, batch_size=args.batch_size, shuffle=True)\n",
        "    bidx = 0\n",
        "    while bidx < n_batchs:\n",
        "        ############################\n",
        "        # (1) Update D network\n",
        "        ############################\n",
        "        for p in netD.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        for diters in range(args.CRITIC_ITERS):\n",
        "            # clamp parameters of NetD encoder to a cube\n",
        "            for p in netD.rnn_enc_layer.parameters():\n",
        "                p.data.clamp_(-args.weight_clip, args.weight_clip)\n",
        "            if bidx == n_batchs:\n",
        "                break\n",
        "\n",
        "            inputs = next(trn_loader)\n",
        "            X_p, X_f, Y_true = inputs[0], inputs[1], inputs[2]\n",
        "            batch_size = X_p.size(0)\n",
        "            bidx += 1\n",
        "\n",
        "            # real data\n",
        "            X_p_enc, X_p_dec = netD(X_p)\n",
        "            X_f_enc, X_f_dec = netD(X_f)\n",
        "\n",
        "            # fake data\n",
        "            noise = torch.cuda.FloatTensor(1, batch_size, args.RNN_hid_dim).normal_(0, 1)\n",
        "            noise = Variable(noise, volatile=True) # total freeze netG\n",
        "            Y_f = Variable(netG(X_p, X_f, noise).data)\n",
        "            Y_f_enc, Y_f_dec = netD(Y_f)\n",
        "\n",
        "            # batchwise MMD2 loss between X_f and Y_f\n",
        "            D_mmd2 = batch_mmd2_loss(X_f_enc, Y_f_enc, sigma_var)\n",
        "\n",
        "            # batchwise MMD loss between X_p and X_f\n",
        "            mmd2_real = batch_mmd2_loss(X_p_enc, X_f_enc, sigma_var)\n",
        "\n",
        "            # reconstruction loss\n",
        "            real_L2_loss = torch.mean((X_f - X_f_dec)**2)\n",
        "            #real_L2_loss = torch.mean((X_p - X_p_dec)**2)\n",
        "            fake_L2_loss = torch.mean((Y_f - Y_f_dec)**2)\n",
        "            #fake_L2_loss = torch.mean((Y_f - Y_f_dec)**2) * 0.0\n",
        "\n",
        "            # update netD\n",
        "            netD.zero_grad()\n",
        "            lossD = D_mmd2.mean() - lambda_ae * (real_L2_loss + fake_L2_loss) - lambda_real * mmd2_real.mean()\n",
        "            #lossD = 0.0 * D_mmd2.mean() - lambda_ae * (real_L2_loss + fake_L2_loss) - lambda_real * mmd2_real.mean()\n",
        "            #lossD = -real_L2_loss\n",
        "            lossD.backward(mone)\n",
        "            optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network\n",
        "        ############################\n",
        "\n",
        "        \n",
        "\n",
        "        for p in netD.parameters():\n",
        "            p.requires_grad = False  # to avoid computation\n",
        "\n",
        "        #if bidx == n_batchs:\n",
        "        #    break\n",
        "        print('G network')  \n",
        "        #inputs = next(trn_loader)\n",
        "        X_p, X_f = inputs[0], inputs[1]\n",
        "        batch_size = X_p.size(0)\n",
        "        bidx += 1\n",
        "\n",
        "        # real data\n",
        "        X_f_enc, X_f_dec = netD(X_f)\n",
        "\n",
        "        # fake data\n",
        "        noise = torch.cuda.FloatTensor(1, batch_size, args.RNN_hid_dim).normal_(0, 1)\n",
        "        noise = Variable(noise)\n",
        "        Y_f = netG(X_p, X_f, noise)\n",
        "        Y_f_enc, Y_f_dec = netD(Y_f)\n",
        "\n",
        "        # batchwise MMD2 loss between X_f and Y_f\n",
        "        G_mmd2 = batch_mmd2_loss(X_f_enc, Y_f_enc, sigma_var)\n",
        "\n",
        "        # update netG\n",
        "        netG.zero_grad()\n",
        "        lossG = G_mmd2.mean()\n",
        "        #lossG = 0.0 * G_mmd2.mean()\n",
        "        lossG.backward(one)\n",
        "        optimizerG.step()\n",
        "\n",
        "        #G_mmd2 = Variable(torch.FloatTensor(batch_size).zero_())\n",
        "        gen_iterations += 1\n",
        "\n",
        "        print('[%5d/%5d] [%5d/%5d] [%6d] D_mmd2 %.4e G_mmd2 %.4e mmd2_real %.4e real_L2 %.6f fake_L2 %.6f'\n",
        "              % (epoch, args.max_iter, bidx, n_batchs, gen_iterations,\n",
        "                 D_mmd2.mean().data[0], G_mmd2.mean().data[0], mmd2_real.mean().data[0],\n",
        "                 real_L2_loss.data[0], fake_L2_loss.data[0]))\n",
        "\n",
        "        if gen_iterations % args.eval_freq == 0:\n",
        "            # ========= Main block for evaluate MMD(X_p_enc, X_f_enc) on RNN codespace  =========#\n",
        "            val_dict = valid_epoch(Data, Data.val_set, netD, args.batch_size, Y_val, L_val)\n",
        "            tst_dict = valid_epoch(Data, Data.tst_set, netD, args.batch_size, Y_tst, L_tst)\n",
        "            total_time = time.time() - start_time\n",
        "            print('iter %4d tm %4.2fm val_mse %.1f val_mae %.1f val_auc %.6f'\n",
        "                    % (epoch, total_time / 60.0, val_dict['mse'], val_dict['mae'], val_dict['auc']), end='')\n",
        "\n",
        "            print (\" tst_mse %.1f tst_mae %.1f tst_auc %.6f\" % (tst_dict['mse'], tst_dict['mae'], tst_dict['auc']), end='')\n",
        "\n",
        "            assert(np.isnan(val_dict['auc']) != True)\n",
        "            #if val_dict['auc'] > best_val_auc:\n",
        "            #if val_dict['auc'] > best_val_auc and mmd2_real.mean().data[0] < best_mmd_real:\n",
        "            if mmd2_real.mean().data[0] < best_mmd_real:\n",
        "                best_mmd_real = mmd2_real.mean().data[0]\n",
        "                best_val_mae = val_dict['mae']\n",
        "                best_val_auc = val_dict['auc']\n",
        "                best_tst_auc = tst_dict['auc']\n",
        "                best_epoch = epoch\n",
        "                save_pred_name = '%s/pred.pkl' % (args.save_path)\n",
        "                with open(save_pred_name, 'wb') as f:\n",
        "                    pickle.dump(tst_dict, f)\n",
        "                torch.save(netG.state_dict(), '%s/netG.pkl' % (args.save_path))\n",
        "                torch.save(netD.state_dict(), '%s/netD.pkl' % (args.save_path))\n",
        "            print(\" [best_val_auc %.6f best_tst_auc %.6f best_epoch %3d]\" % (best_val_auc, best_tst_auc, best_epoch))\n",
        "\n",
        "        # stopping condition\n",
        "        #if best_mmd_real < 1e-4:\n",
        "        if mmd2_real.mean().data[0] < 1e-5:\n",
        "            exit(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "NTb_Kei2L2ID",
        "outputId": "89778ef7-7d18-4051-f1d6-1af5cdbfda66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_batchs 5 batch_size 128\n",
            "start training: lambda_ae 0.001 lambda_real 0.1 weight_clip 0.1\n",
            "G network\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-90-9fe2a6c7bc76>:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  noise = Variable(noise, volatile=True) # total freeze netG\n",
            "<ipython-input-85-dd83be09ac40>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  sigma_samples = F.softmax(U * gumbel_lmd).matmul(sigma_var)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-9fe2a6c7bc76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m         print('[%5d/%5d] [%5d/%5d] [%6d] D_mmd2 %.4e G_mmd2 %.4e mmd2_real %.4e real_L2 %.6f fake_L2 %.6f'\n\u001b[1;32m    113\u001b[0m               % (epoch, args.max_iter, bidx, n_batchs, gen_iterations,\n\u001b[0;32m--> 114\u001b[0;31m                  \u001b[0mD_mmd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_mmd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmd2_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                  real_L2_loss.data[0], fake_L2_loss.data[0]))\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ]
    }
  ]
}